{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4_Hdn5JwVQhj"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "UQb_DSTZeRgp",
    "outputId": "e0e87533-2980-4647-a148-6dce37c93568"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9sYmB--VecVF"
   },
   "source": [
    "#Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "OwJ9zABjVczG"
   },
   "outputs": [],
   "source": [
    "new_keywords = pd.read_csv('/content/drive/MyDrive/nearest-phrases/new-keywords (1).csv')\n",
    "app_topics = pd.read_csv('/content/drive/MyDrive/nearest-phrases/list-of-approved-topics (1).csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "kcXtWvFof3EW",
    "outputId": "3106be7e-064b-4549-a4b7-6f6bde33ebb2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>storytelling interview questions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sales certifications</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>consultant career</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>investigator cover letter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hard lines vs. soft lines</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Name\n",
       "0  storytelling interview questions\n",
       "1              sales certifications\n",
       "2                 consultant career\n",
       "3         investigator cover letter\n",
       "4         hard lines vs. soft lines"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_keywords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "MuYXG86tf5nR",
    "outputId": "4fc4ac8c-aa85-4c99-dbcf-90fd511f1de5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how to write a vision statement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>performance evaluation comments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>team leader qualities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>creative jobs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>employee performance review template</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      0\n",
       "0       how to write a vision statement\n",
       "1       performance evaluation comments\n",
       "2                 team leader qualities\n",
       "3                         creative jobs\n",
       "4  employee performance review template"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_topics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tgiq7dMZhfQx"
   },
   "source": [
    "#Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "ybOdI404hiX_",
    "outputId": "c6c4ff7e-8ae5-49be-ea34-84b4691d538a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "stop_words = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "btaHdIMKVkeR"
   },
   "outputs": [],
   "source": [
    "\n",
    "def initial_clean(text):\n",
    "  '''\n",
    "  function to lower and tokenize the text\n",
    "  '''\n",
    "\n",
    "  text = text.lower()\n",
    "  text = nltk.word_tokenize(text)\n",
    "  return text\n",
    "\n",
    "\n",
    "def remove_stop_words(text):\n",
    "  '''\n",
    "   function to remove stop_words\n",
    "  '''\n",
    "  return [word for word in text if word not in stop_words]\n",
    "\n",
    "\n",
    "  \n",
    "def stem_words(text):\n",
    "  '''\n",
    "   function to lemmatize \n",
    "\n",
    "  '''\n",
    "\n",
    "  try:\n",
    "      text = [lemmatizer.lemmatize(word) for word in text]\n",
    "      text = [word for word in text if len(word) > 1] \n",
    "  except IndexError:\n",
    "      pass\n",
    "  return text\n",
    "\n",
    "\n",
    "def apply_all(text):\n",
    "  \n",
    "  return stem_words(remove_stop_words(initial_clean(text)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPbMKZ29iwd-"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "9XTgX5oFV-CS"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['tokenized'] = new_keywords['Name'].apply(apply_all) \n",
    "d = app_topics[0].apply(apply_all) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "j5scAjPkWHu2"
   },
   "outputs": [],
   "source": [
    "#combining and adding both the data to make corpus\n",
    "new_df = pd.DataFrame()\n",
    "new_df['tokenized']=d\n",
    "com = pd.concat([df, new_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "id": "3d6aQD_FWMEw",
    "outputId": "95287c4b-f25f-4659-e8d1-bbdaeedcf594"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[storytelling, interview, question]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[sale, certification]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[consultant, career]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[investigator, cover, letter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[hard, line, vs., soft, line]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>[become, process, server]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>[become, video, editor]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>[become, immigration, lawyer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>[become, medical, esthetician]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>[long, take, become, occupational, therapist]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>227 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         tokenized\n",
       "0              [storytelling, interview, question]\n",
       "1                            [sale, certification]\n",
       "2                             [consultant, career]\n",
       "3                    [investigator, cover, letter]\n",
       "4                    [hard, line, vs., soft, line]\n",
       "..                                             ...\n",
       "222                      [become, process, server]\n",
       "223                        [become, video, editor]\n",
       "224                  [become, immigration, lawyer]\n",
       "225                 [become, medical, esthetician]\n",
       "226  [long, take, become, occupational, therapist]\n",
       "\n",
       "[227 rows x 1 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lz65BkmNjBnj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aK4fgjVajBc8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbBsBmNJjCiQ"
   },
   "source": [
    "#Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gwf7O-IhkPEl"
   },
   "source": [
    "## Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xaV-s22kRmi"
   },
   "source": [
    "Universal Sentence Encoder is sentence embedding techniques that has been proposed by Google.\n",
    "sentence embeddings we generate can be used for multiple tasks like sentiment analysis, text classification, sentence similarity, etc\n",
    "\n",
    "This encoder is based on two encoder models and we can use either of the two\n",
    "* Transformer\n",
    "*Deep Averaging Network(DAN). \n",
    "\n",
    "## basic flow:\n",
    "\n",
    "1. Tokenize the sentences after converting them to lowercase\n",
    "2. Depending on the type of encoder, the sentence gets converted to a 512-dimensional vector\n",
    "If we use the transformer, it is similar to the encoder module of the transformer architecture and uses the self-attention mechanism.\n",
    "The DAN option computes the unigram and bigram embeddings first and then averages them to get a single embedding. This is then passed to a deep neural network to get a final sentence embedding of 512 dimensions.\n",
    "3. These sentence embeddings are then used for various unsupervised and supervised tasks like Skipthoughts, NLI, etc. The trained model is then again reused to generate a new 512 dimension sentence embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vPb9dXbgWMBf"
   },
   "outputs": [],
   "source": [
    "!pip3 install --upgrade tensorflow-gpu\n",
    "# Install TF-Hub.\n",
    "!pip3 install tensorflow-hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCL4cBxKlKLw"
   },
   "source": [
    "The model is available to us via the TFHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pCIx0rGtWL_S"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" \n",
    "model = hub.load(module_url)\n",
    "print (\"module %s loaded\" % module_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0877jCSlSFq"
   },
   "source": [
    "we will generate embeddings for our sentence list as well as for our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "d4LXbd3bWppS"
   },
   "outputs": [],
   "source": [
    "#make sentence corpus \n",
    "s1 = app_topics[0].apply(apply_all).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yF8Q2RYoWL84"
   },
   "outputs": [],
   "source": [
    "sentence_embeddings = model(s1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KfqsPOdtW07V"
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpaccB_RndrT"
   },
   "source": [
    "finding similarity using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nhdjUq4wWwso"
   },
   "outputs": [],
   "source": [
    "for query in tqdm(new_keywords['Name'].values):\n",
    "  print(query)\n",
    "  if query in d:\n",
    "    continue\n",
    "  else:\n",
    "    query_vec = model([query])[0]\n",
    "    r =[]\n",
    "    for sent in s1:\n",
    "    \n",
    "      sim = cosine(query_vec, model([sent])[0])\n",
    "      r.append((sent,sim))\n",
    "      r.sort(key = lambda x: x[1])\n",
    "      r = r[-5:]\n",
    "\n",
    "    d[query] = r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CLRZAZQmK_o"
   },
   "source": [
    "#create dataframe of similar topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "attopAqPj3Yu"
   },
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(d.items())\n",
    "final_df.columns = ['keyword','approved_topics']\n",
    "final_df['approved_topics']=final_df['approved_topics'].apply(lambda x: \"| \".join([i[0] for i in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TUj07w3mm9Y-"
   },
   "outputs": [],
   "source": [
    "final_df.to_csv('approved_topics.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "app_topics.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
